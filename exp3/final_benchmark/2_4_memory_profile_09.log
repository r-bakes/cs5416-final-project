[CONFIG] GPU not available, using CPU
Loading LLM model...
LLM model loaded on cpu with dtype torch.float32!
============================================================
LLM SERVICE
============================================================
Node: 1
Port: 8009
Model: Qwen/Qwen2.5-0.5B-Instruct
Device: cpu
============================================================
 * Serving Flask app '04_llm_service'
 * Debug mode: off

[TIMING] _generate_responses_batch - START
Filename: /home/hy676/cs5416-final-project/exp3/04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    33   2401.1 MiB   2401.1 MiB           1   @profile_with_timing
    34                                         @profile
    35                                         def _generate_responses_batch(
    36                                             queries: list[str], documents_batch: list[list[dict]]
    37                                         ) -> list[str]:
    38                                             """Step 6: Generate LLM responses for each query in the batch"""
    39   2401.1 MiB      0.0 MiB           1       responses = []
    40   2443.2 MiB      0.0 MiB           5       for query, documents in zip(queries, documents_batch):
    41   2443.2 MiB      0.0 MiB           8           context = "\n".join(
    42   2443.2 MiB      0.0 MiB          24               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    43                                                 )
    44   2443.2 MiB      0.0 MiB           4           messages = [
    45   2443.2 MiB      0.0 MiB           4               {
    46   2443.2 MiB      0.0 MiB           4                   "role": "system",
    47   2443.2 MiB      0.0 MiB           4                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    48                                                     },
    49   2443.2 MiB      0.0 MiB           4               {
    50   2443.2 MiB      0.0 MiB           4                   "role": "user",
    51   2443.2 MiB      0.0 MiB           4                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    52                                                     },
    53                                                 ]
    54   2443.2 MiB      0.9 MiB           8           text = tokenizer.apply_chat_template(
    55   2443.2 MiB      0.0 MiB           4               messages, tokenize=False, add_generation_prompt=True
    56                                                 )
    57   2443.2 MiB      0.0 MiB           4           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    58   2443.2 MiB     41.2 MiB          12           generated_ids = model.generate(
    59   2443.2 MiB      0.0 MiB           4               **model_inputs,
    60   2443.2 MiB      0.0 MiB           4               max_new_tokens=CONFIG["max_tokens"],
    61   2443.2 MiB      0.0 MiB           4               temperature=0.01,
    62   2443.2 MiB      0.0 MiB           4               pad_token_id=tokenizer.eos_token_id,
    63                                                 )
    64   2443.2 MiB      0.0 MiB          24           generated_ids = [
    65   2443.2 MiB      0.0 MiB           4               output_ids[len(input_ids) :]
    66   2443.2 MiB      0.0 MiB           8               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    67                                                 ]
    68   2443.2 MiB      0.0 MiB           4           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    69   2443.2 MiB      0.0 MiB           4           responses.append(response)
    70   2443.2 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 24.05s)

[TIMING] _generate_responses_batch - START
Filename: /home/hy676/cs5416-final-project/exp3/04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    33   2444.2 MiB   2444.2 MiB           1   @profile_with_timing
    34                                         @profile
    35                                         def _generate_responses_batch(
    36                                             queries: list[str], documents_batch: list[list[dict]]
    37                                         ) -> list[str]:
    38                                             """Step 6: Generate LLM responses for each query in the batch"""
    39   2444.2 MiB      0.0 MiB           1       responses = []
    40   2471.5 MiB      0.0 MiB           5       for query, documents in zip(queries, documents_batch):
    41   2468.9 MiB      0.0 MiB           8           context = "\n".join(
    42   2468.9 MiB      0.0 MiB          24               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    43                                                 )
    44   2468.9 MiB      0.0 MiB           4           messages = [
    45   2468.9 MiB      0.0 MiB           4               {
    46   2468.9 MiB      0.0 MiB           4                   "role": "system",
    47   2468.9 MiB      0.0 MiB           4                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    48                                                     },
    49   2468.9 MiB      0.0 MiB           4               {
    50   2468.9 MiB      0.0 MiB           4                   "role": "user",
    51   2468.9 MiB      0.0 MiB           4                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    52                                                     },
    53                                                 ]
    54   2468.9 MiB      0.0 MiB           8           text = tokenizer.apply_chat_template(
    55   2468.9 MiB      0.0 MiB           4               messages, tokenize=False, add_generation_prompt=True
    56                                                 )
    57   2468.9 MiB      0.0 MiB           4           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    58   2471.5 MiB     27.3 MiB          12           generated_ids = model.generate(
    59   2468.9 MiB      0.0 MiB           4               **model_inputs,
    60   2468.9 MiB      0.0 MiB           4               max_new_tokens=CONFIG["max_tokens"],
    61   2468.9 MiB      0.0 MiB           4               temperature=0.01,
    62   2468.9 MiB      0.0 MiB           4               pad_token_id=tokenizer.eos_token_id,
    63                                                 )
    64   2471.5 MiB      0.0 MiB          24           generated_ids = [
    65   2471.5 MiB      0.0 MiB           4               output_ids[len(input_ids) :]
    66   2471.5 MiB      0.0 MiB           8               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    67                                                 ]
    68   2471.5 MiB      0.0 MiB           4           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    69   2471.5 MiB      0.0 MiB           4           responses.append(response)
    70   2471.5 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 24.90s)

[TIMING] _generate_responses_batch - START
Filename: /home/hy676/cs5416-final-project/exp3/04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    33   2472.4 MiB   2472.4 MiB           1   @profile_with_timing
    34                                         @profile
    35                                         def _generate_responses_batch(
    36                                             queries: list[str], documents_batch: list[list[dict]]
    37                                         ) -> list[str]:
    38                                             """Step 6: Generate LLM responses for each query in the batch"""
    39   2472.4 MiB      0.0 MiB           1       responses = []
    40   2481.4 MiB      0.0 MiB           5       for query, documents in zip(queries, documents_batch):
    41   2481.4 MiB      0.0 MiB           8           context = "\n".join(
    42   2481.4 MiB      0.0 MiB          24               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    43                                                 )
    44   2481.4 MiB      0.0 MiB           4           messages = [
    45   2481.4 MiB      0.0 MiB           4               {
    46   2481.4 MiB      0.0 MiB           4                   "role": "system",
    47   2481.4 MiB      0.0 MiB           4                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    48                                                     },
    49   2481.4 MiB      0.0 MiB           4               {
    50   2481.4 MiB      0.0 MiB           4                   "role": "user",
    51   2481.4 MiB      0.0 MiB           4                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    52                                                     },
    53                                                 ]
    54   2481.4 MiB      0.0 MiB           8           text = tokenizer.apply_chat_template(
    55   2481.4 MiB      0.0 MiB           4               messages, tokenize=False, add_generation_prompt=True
    56                                                 )
    57   2481.4 MiB      0.0 MiB           4           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    58   2481.4 MiB      9.0 MiB          12           generated_ids = model.generate(
    59   2481.4 MiB      0.0 MiB           4               **model_inputs,
    60   2481.4 MiB      0.0 MiB           4               max_new_tokens=CONFIG["max_tokens"],
    61   2481.4 MiB      0.0 MiB           4               temperature=0.01,
    62   2481.4 MiB      0.0 MiB           4               pad_token_id=tokenizer.eos_token_id,
    63                                                 )
    64   2481.4 MiB      0.0 MiB          24           generated_ids = [
    65   2481.4 MiB      0.0 MiB           4               output_ids[len(input_ids) :]
    66   2481.4 MiB      0.0 MiB           8               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    67                                                 ]
    68   2481.4 MiB      0.0 MiB           4           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    69   2481.4 MiB      0.0 MiB           4           responses.append(response)
    70   2481.4 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 16.66s)
