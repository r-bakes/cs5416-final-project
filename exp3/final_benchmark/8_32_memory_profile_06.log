[CONFIG] GPU not available, using CPU
============================================================
SENTIMENT & SAFETY SERVICE
============================================================
Node: 0
Port: 9006
Sentiment Model: nlptown/bert-base-multilingual-uncased-sentiment
Safety Model: unitary/toxic-bert
Device: cpu
============================================================
 * Serving Flask app '05_sentiment_and_safety_service'
 * Debug mode: off

[TIMING] _analyze_sentiment_batch - START
Filename: /home/rb972/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26    819.4 MiB    819.4 MiB           1   @profile_with_timing
    27                                         @profile
    28                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    29                                             """Step 7: Analyze sentiment for each generated response"""
    30    819.4 MiB      0.0 MiB          15       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    31   1147.4 MiB    327.9 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    32   1147.4 MiB      0.0 MiB           1       sentiment_map = {
    33   1147.4 MiB      0.0 MiB           1           "1 star": "very negative",
    34   1147.4 MiB      0.0 MiB           1           "2 stars": "negative",
    35   1147.4 MiB      0.0 MiB           1           "3 stars": "neutral",
    36   1147.4 MiB      0.0 MiB           1           "4 stars": "positive",
    37   1147.4 MiB      0.0 MiB           1           "5 stars": "very positive",
    38                                             }
    39   1147.4 MiB      0.0 MiB           1       sentiments = []
    40   1147.4 MiB      0.0 MiB          13       for result in raw_results:
    41   1147.4 MiB      0.0 MiB          12           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    42   1147.4 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 1.52s)

[TIMING] _filter_response_safety_batch - START
Filename: /home/rb972/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    45   1147.4 MiB   1147.4 MiB           1   @profile_with_timing
    46                                         @profile
    47                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    48                                             """Step 8: Filter responses for safety for each entry in the batch"""
    49   1147.4 MiB      0.0 MiB          15       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    50   1464.4 MiB    317.0 MiB           1       raw_results = safety_classifier(truncated_texts)
    51   1464.4 MiB      0.0 MiB           1       toxicity_flags = []
    52   1464.4 MiB      0.0 MiB          13       for result in raw_results:
    53   1464.4 MiB      0.0 MiB          12           toxicity_flags.append(result["score"] > 0.5)
    54   1464.4 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 1.49s)

[TIMING] _analyze_sentiment_batch - START
Filename: /home/rb972/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26   1472.8 MiB   1472.8 MiB           1   @profile_with_timing
    27                                         @profile
    28                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    29                                             """Step 7: Analyze sentiment for each generated response"""
    30   1472.8 MiB      0.0 MiB          16       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    31   1490.7 MiB     17.9 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    32   1490.7 MiB      0.0 MiB           1       sentiment_map = {
    33   1490.7 MiB      0.0 MiB           1           "1 star": "very negative",
    34   1490.7 MiB      0.0 MiB           1           "2 stars": "negative",
    35   1490.7 MiB      0.0 MiB           1           "3 stars": "neutral",
    36   1490.7 MiB      0.0 MiB           1           "4 stars": "positive",
    37   1490.7 MiB      0.0 MiB           1           "5 stars": "very positive",
    38                                             }
    39   1490.7 MiB      0.0 MiB           1       sentiments = []
    40   1490.7 MiB      0.0 MiB          14       for result in raw_results:
    41   1490.7 MiB      0.0 MiB          13           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    42   1490.7 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 3.02s)

[TIMING] _filter_response_safety_batch - START
Filename: /home/rb972/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    45   1490.7 MiB   1490.7 MiB           1   @profile_with_timing
    46                                         @profile
    47                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    48                                             """Step 8: Filter responses for safety for each entry in the batch"""
    49   1490.7 MiB      0.0 MiB          16       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    50   1491.5 MiB      0.8 MiB           1       raw_results = safety_classifier(truncated_texts)
    51   1491.5 MiB      0.0 MiB           1       toxicity_flags = []
    52   1491.5 MiB      0.0 MiB          14       for result in raw_results:
    53   1491.5 MiB      0.0 MiB          13           toxicity_flags.append(result["score"] > 0.5)
    54   1491.5 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 2.33s)

[TIMING] _analyze_sentiment_batch - START
Filename: /home/rb972/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26   1494.0 MiB   1494.0 MiB           1   @profile_with_timing
    27                                         @profile
    28                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    29                                             """Step 7: Analyze sentiment for each generated response"""
    30   1494.0 MiB      0.0 MiB          16       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    31   1498.6 MiB      4.6 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    32   1498.6 MiB      0.0 MiB           1       sentiment_map = {
    33   1498.6 MiB      0.0 MiB           1           "1 star": "very negative",
    34   1498.6 MiB      0.0 MiB           1           "2 stars": "negative",
    35   1498.6 MiB      0.0 MiB           1           "3 stars": "neutral",
    36   1498.6 MiB      0.0 MiB           1           "4 stars": "positive",
    37   1498.6 MiB      0.0 MiB           1           "5 stars": "very positive",
    38                                             }
    39   1498.6 MiB      0.0 MiB           1       sentiments = []
    40   1498.6 MiB      0.0 MiB          14       for result in raw_results:
    41   1498.6 MiB      0.0 MiB          13           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    42   1498.6 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 2.79s)

[TIMING] _filter_response_safety_batch - START
Filename: /home/rb972/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    45   1498.6 MiB   1498.6 MiB           1   @profile_with_timing
    46                                         @profile
    47                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    48                                             """Step 8: Filter responses for safety for each entry in the batch"""
    49   1498.6 MiB      0.0 MiB          16       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    50   1498.6 MiB      0.0 MiB           1       raw_results = safety_classifier(truncated_texts)
    51   1498.6 MiB      0.0 MiB           1       toxicity_flags = []
    52   1498.6 MiB      0.0 MiB          14       for result in raw_results:
    53   1498.6 MiB      0.0 MiB          13           toxicity_flags.append(result["score"] > 0.5)
    54   1498.6 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 2.60s)

[TIMING] _analyze_sentiment_batch - START
Filename: /home/rb972/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26   1501.4 MiB   1501.4 MiB           1   @profile_with_timing
    27                                         @profile
    28                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    29                                             """Step 7: Analyze sentiment for each generated response"""
    30   1501.4 MiB      0.0 MiB          16       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    31   1501.4 MiB      0.0 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    32   1501.4 MiB      0.0 MiB           1       sentiment_map = {
    33   1501.4 MiB      0.0 MiB           1           "1 star": "very negative",
    34   1501.4 MiB      0.0 MiB           1           "2 stars": "negative",
    35   1501.4 MiB      0.0 MiB           1           "3 stars": "neutral",
    36   1501.4 MiB      0.0 MiB           1           "4 stars": "positive",
    37   1501.4 MiB      0.0 MiB           1           "5 stars": "very positive",
    38                                             }
    39   1501.4 MiB      0.0 MiB           1       sentiments = []
    40   1501.4 MiB      0.0 MiB          14       for result in raw_results:
    41   1501.4 MiB      0.0 MiB          13           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    42   1501.4 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 1.30s)

[TIMING] _filter_response_safety_batch - START
Filename: /home/rb972/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    45   1501.4 MiB   1501.4 MiB           1   @profile_with_timing
    46                                         @profile
    47                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    48                                             """Step 8: Filter responses for safety for each entry in the batch"""
    49   1501.4 MiB      0.0 MiB          16       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    50   1501.4 MiB      0.0 MiB           1       raw_results = safety_classifier(truncated_texts)
    51   1501.4 MiB      0.0 MiB           1       toxicity_flags = []
    52   1501.4 MiB      0.0 MiB          14       for result in raw_results:
    53   1501.4 MiB      0.0 MiB          13           toxicity_flags.append(result["score"] > 0.5)
    54   1501.4 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 1.22s)
