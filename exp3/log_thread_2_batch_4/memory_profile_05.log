============================================================
SENTIMENT & SAFETY SERVICE
============================================================
Node: 0
Port: 8005
Sentiment Model: nlptown/bert-base-multilingual-uncased-sentiment
Safety Model: unitary/toxic-bert
============================================================
 * Serving Flask app '05_sentiment_and_safety_service'
 * Debug mode: off

[TIMING] _analyze_sentiment_batch - START
Filename: /home/yy2394/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26    600.8 MiB    600.8 MiB           1   @profile_with_timing
    27                                         @profile
    28                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    29                                             """Step 7: Analyze sentiment for each generated response"""
    30    600.8 MiB      0.0 MiB           7       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    31    783.4 MiB    182.6 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    32    783.4 MiB      0.0 MiB           1       sentiment_map = {
    33    783.4 MiB      0.0 MiB           1           "1 star": "very negative",
    34    783.4 MiB      0.0 MiB           1           "2 stars": "negative",
    35    783.4 MiB      0.0 MiB           1           "3 stars": "neutral",
    36    783.4 MiB      0.0 MiB           1           "4 stars": "positive",
    37    783.4 MiB      0.0 MiB           1           "5 stars": "very positive",
    38                                             }
    39    783.4 MiB      0.0 MiB           1       sentiments = []
    40    783.4 MiB      0.0 MiB           5       for result in raw_results:
    41    783.4 MiB      0.0 MiB           4           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    42    783.4 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 8.71s)

[TIMING] _filter_response_safety_batch - START
Filename: /home/yy2394/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    45    783.4 MiB    783.4 MiB           1   @profile_with_timing
    46                                         @profile
    47                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    48                                             """Step 8: Filter responses for safety for each entry in the batch"""
    49    783.4 MiB      0.0 MiB           7       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    50    988.8 MiB    205.3 MiB           1       raw_results = safety_classifier(truncated_texts)
    51    988.8 MiB      0.0 MiB           1       toxicity_flags = []
    52    988.8 MiB      0.0 MiB           5       for result in raw_results:
    53    988.8 MiB      0.0 MiB           4           toxicity_flags.append(result["score"] > 0.5)
    54    988.8 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 7.08s)

[TIMING] _analyze_sentiment_batch - START
Filename: /home/yy2394/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26    978.0 MiB    978.0 MiB           1   @profile_with_timing
    27                                         @profile
    28                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    29                                             """Step 7: Analyze sentiment for each generated response"""
    30    978.0 MiB      0.0 MiB           7       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    31    984.3 MiB      6.3 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    32    984.3 MiB      0.0 MiB           1       sentiment_map = {
    33    984.3 MiB      0.0 MiB           1           "1 star": "very negative",
    34    984.3 MiB      0.0 MiB           1           "2 stars": "negative",
    35    984.3 MiB      0.0 MiB           1           "3 stars": "neutral",
    36    984.3 MiB      0.0 MiB           1           "4 stars": "positive",
    37    984.3 MiB      0.0 MiB           1           "5 stars": "very positive",
    38                                             }
    39    984.3 MiB      0.0 MiB           1       sentiments = []
    40    984.3 MiB      0.0 MiB           5       for result in raw_results:
    41    984.3 MiB      0.0 MiB           4           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    42    984.3 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 0.34s)

[TIMING] _filter_response_safety_batch - START
Filename: /home/yy2394/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    45    984.3 MiB    984.3 MiB           1   @profile_with_timing
    46                                         @profile
    47                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    48                                             """Step 8: Filter responses for safety for each entry in the batch"""
    49    984.3 MiB      0.0 MiB           7       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    50    984.3 MiB      0.0 MiB           1       raw_results = safety_classifier(truncated_texts)
    51    984.3 MiB      0.0 MiB           1       toxicity_flags = []
    52    984.3 MiB      0.0 MiB           5       for result in raw_results:
    53    984.3 MiB      0.0 MiB           4           toxicity_flags.append(result["score"] > 0.5)
    54    984.3 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 0.32s)

[TIMING] _analyze_sentiment_batch - START
Filename: /home/yy2394/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26    985.5 MiB    985.5 MiB           1   @profile_with_timing
    27                                         @profile
    28                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    29                                             """Step 7: Analyze sentiment for each generated response"""
    30    985.5 MiB      0.0 MiB           7       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    31    989.2 MiB      3.7 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    32    989.2 MiB      0.0 MiB           1       sentiment_map = {
    33    989.2 MiB      0.0 MiB           1           "1 star": "very negative",
    34    989.2 MiB      0.0 MiB           1           "2 stars": "negative",
    35    989.2 MiB      0.0 MiB           1           "3 stars": "neutral",
    36    989.2 MiB      0.0 MiB           1           "4 stars": "positive",
    37    989.2 MiB      0.0 MiB           1           "5 stars": "very positive",
    38                                             }
    39    989.2 MiB      0.0 MiB           1       sentiments = []
    40    989.2 MiB      0.0 MiB           5       for result in raw_results:
    41    989.2 MiB      0.0 MiB           4           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    42    989.2 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 0.73s)

[TIMING] _filter_response_safety_batch - START
Filename: /home/yy2394/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    45    989.2 MiB    989.2 MiB           1   @profile_with_timing
    46                                         @profile
    47                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    48                                             """Step 8: Filter responses for safety for each entry in the batch"""
    49    989.2 MiB      0.0 MiB           7       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    50    990.2 MiB      1.0 MiB           1       raw_results = safety_classifier(truncated_texts)
    51    990.2 MiB      0.0 MiB           1       toxicity_flags = []
    52    990.2 MiB      0.0 MiB           5       for result in raw_results:
    53    990.2 MiB      0.0 MiB           4           toxicity_flags.append(result["score"] > 0.5)
    54    990.2 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 0.55s)

[TIMING] _analyze_sentiment_batch - START
Filename: /home/yy2394/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26    993.9 MiB    993.9 MiB           1   @profile_with_timing
    27                                         @profile
    28                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    29                                             """Step 7: Analyze sentiment for each generated response"""
    30    993.9 MiB      0.0 MiB           5       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    31    993.9 MiB      0.0 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    32    993.9 MiB      0.0 MiB           1       sentiment_map = {
    33    993.9 MiB      0.0 MiB           1           "1 star": "very negative",
    34    993.9 MiB      0.0 MiB           1           "2 stars": "negative",
    35    993.9 MiB      0.0 MiB           1           "3 stars": "neutral",
    36    993.9 MiB      0.0 MiB           1           "4 stars": "positive",
    37    993.9 MiB      0.0 MiB           1           "5 stars": "very positive",
    38                                             }
    39    993.9 MiB      0.0 MiB           1       sentiments = []
    40    993.9 MiB      0.0 MiB           3       for result in raw_results:
    41    993.9 MiB      0.0 MiB           2           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    42    993.9 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 0.23s)

[TIMING] _filter_response_safety_batch - START
Filename: /home/yy2394/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    45    993.9 MiB    993.9 MiB           1   @profile_with_timing
    46                                         @profile
    47                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    48                                             """Step 8: Filter responses for safety for each entry in the batch"""
    49    993.9 MiB      0.0 MiB           5       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    50    993.9 MiB      0.0 MiB           1       raw_results = safety_classifier(truncated_texts)
    51    993.9 MiB      0.0 MiB           1       toxicity_flags = []
    52    993.9 MiB      0.0 MiB           3       for result in raw_results:
    53    993.9 MiB      0.0 MiB           2           toxicity_flags.append(result["score"] > 0.5)
    54    993.9 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 0.21s)
