Loading LLM model...
LLM model loaded!
============================================================
LLM SERVICE
============================================================
Node: 1
Port: 8009
Model: Qwen/Qwen2.5-0.5B-Instruct
============================================================
 * Serving Flask app '04_llm_service'
 * Debug mode: off

[TIMING] _generate_responses_batch - START
Filename: /home/hy676/cs5416-final-project/exp3/04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    31   1715.3 MiB   1715.3 MiB           1   @profile_with_timing
    32                                         @profile
    33                                         def _generate_responses_batch(
    34                                             queries: list[str], documents_batch: list[list[dict]]
    35                                         ) -> list[str]:
    36                                             """Step 6: Generate LLM responses for each query in the batch"""
    37   1715.3 MiB      0.0 MiB           1       responses = []
    38   1717.7 MiB      0.0 MiB           5       for query, documents in zip(queries, documents_batch):
    39   1716.7 MiB    -11.4 MiB           8           context = "\n".join(
    40   1716.7 MiB    -34.2 MiB          24               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    41                                                 )
    42   1716.7 MiB     -5.7 MiB           4           messages = [
    43   1716.7 MiB     -5.7 MiB           4               {
    44   1716.7 MiB     -5.7 MiB           4                   "role": "system",
    45   1716.7 MiB     -5.7 MiB           4                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    46                                                     },
    47   1716.7 MiB     -5.7 MiB           4               {
    48   1716.7 MiB     -5.7 MiB           4                   "role": "user",
    49   1716.7 MiB     -5.7 MiB           4                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    50                                                     },
    51                                                 ]
    52   1716.7 MiB    -10.6 MiB           8           text = tokenizer.apply_chat_template(
    53   1716.7 MiB     -7.3 MiB           4               messages, tokenize=False, add_generation_prompt=True
    54                                                 )
    55   1716.7 MiB     -6.9 MiB           4           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    56   1717.7 MiB    -22.8 MiB          12           generated_ids = model.generate(
    57   1716.7 MiB     -8.0 MiB           4               **model_inputs,
    58   1716.7 MiB     -8.0 MiB           4               max_new_tokens=CONFIG["max_tokens"],
    59   1716.7 MiB     -8.0 MiB           4               temperature=0.01,
    60   1716.7 MiB     -8.0 MiB           4               pad_token_id=tokenizer.eos_token_id,
    61                                                 )
    62   1717.7 MiB     -8.0 MiB          24           generated_ids = [
    63   1717.7 MiB      0.0 MiB           4               output_ids[len(input_ids) :]
    64   1717.7 MiB      0.0 MiB           8               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    65                                                 ]
    66   1717.7 MiB      0.0 MiB           4           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    67   1717.7 MiB      0.0 MiB           4           responses.append(response)
    68   1717.7 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 72.92s)

[TIMING] _generate_responses_batch - START

[TIMING] _generate_responses_batch - START
Filename: /home/hy676/cs5416-final-project/exp3/04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    31   1702.7 MiB   1702.7 MiB           1   @profile_with_timing
    32                                         @profile
    33                                         def _generate_responses_batch(
    34                                             queries: list[str], documents_batch: list[list[dict]]
    35                                         ) -> list[str]:
    36                                             """Step 6: Generate LLM responses for each query in the batch"""
    37   1702.7 MiB      0.0 MiB           1       responses = []
    38   1730.4 MiB      0.0 MiB           5       for query, documents in zip(queries, documents_batch):
    39   1722.7 MiB      0.0 MiB           8           context = "\n".join(
    40   1722.7 MiB      0.0 MiB          24               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    41                                                 )
    42   1722.7 MiB      0.0 MiB           4           messages = [
    43   1722.7 MiB      0.0 MiB           4               {
    44   1722.7 MiB      0.0 MiB           4                   "role": "system",
    45   1722.7 MiB      0.0 MiB           4                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    46                                                     },
    47   1722.7 MiB      0.0 MiB           4               {
    48   1722.7 MiB      0.0 MiB           4                   "role": "user",
    49   1722.7 MiB      0.0 MiB           4                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    50                                                     },
    51                                                 ]
    52   1722.7 MiB      0.0 MiB           8           text = tokenizer.apply_chat_template(
    53   1722.7 MiB      0.0 MiB           4               messages, tokenize=False, add_generation_prompt=True
    54                                                 )
    55   1722.7 MiB      0.0 MiB           4           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    56   1730.4 MiB     27.7 MiB          12           generated_ids = model.generate(
    57   1722.7 MiB      0.0 MiB           4               **model_inputs,
    58   1722.7 MiB      0.0 MiB           4               max_new_tokens=CONFIG["max_tokens"],
    59   1722.7 MiB      0.0 MiB           4               temperature=0.01,
    60   1722.7 MiB      0.0 MiB           4               pad_token_id=tokenizer.eos_token_id,
    61                                                 )
    62   1730.4 MiB      0.0 MiB          24           generated_ids = [
    63   1730.4 MiB      0.0 MiB           4               output_ids[len(input_ids) :]
    64   1730.4 MiB      0.0 MiB           8               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    65                                                 ]
    66   1730.4 MiB      0.0 MiB           4           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    67   1730.4 MiB      0.0 MiB           4           responses.append(response)
    68   1730.4 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 119.71s)
Filename: /home/hy676/cs5416-final-project/exp3/04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    31   1715.5 MiB   1715.5 MiB           1   @profile_with_timing
    32                                         @profile
    33                                         def _generate_responses_batch(
    34                                             queries: list[str], documents_batch: list[list[dict]]
    35                                         ) -> list[str]:
    36                                             """Step 6: Generate LLM responses for each query in the batch"""
    37   1715.5 MiB      0.0 MiB           1       responses = []
    38   1731.9 MiB      0.0 MiB           5       for query, documents in zip(queries, documents_batch):
    39   1731.9 MiB      0.0 MiB           8           context = "\n".join(
    40   1731.9 MiB      0.0 MiB          24               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    41                                                 )
    42   1731.9 MiB      0.0 MiB           4           messages = [
    43   1731.9 MiB      0.0 MiB           4               {
    44   1731.9 MiB      0.0 MiB           4                   "role": "system",
    45   1731.9 MiB      0.0 MiB           4                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    46                                                     },
    47   1731.9 MiB      0.0 MiB           4               {
    48   1731.9 MiB      0.0 MiB           4                   "role": "user",
    49   1731.9 MiB      0.0 MiB           4                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    50                                                     },
    51                                                 ]
    52   1731.9 MiB      0.0 MiB           8           text = tokenizer.apply_chat_template(
    53   1731.9 MiB      0.0 MiB           4               messages, tokenize=False, add_generation_prompt=True
    54                                                 )
    55   1731.9 MiB      0.0 MiB           4           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    56   1731.9 MiB     16.5 MiB          12           generated_ids = model.generate(
    57   1731.9 MiB      0.0 MiB           4               **model_inputs,
    58   1731.9 MiB      0.0 MiB           4               max_new_tokens=CONFIG["max_tokens"],
    59   1731.9 MiB      0.0 MiB           4               temperature=0.01,
    60   1731.9 MiB      0.0 MiB           4               pad_token_id=tokenizer.eos_token_id,
    61                                                 )
    62   1731.9 MiB      0.0 MiB          24           generated_ids = [
    63   1731.9 MiB      0.0 MiB           4               output_ids[len(input_ids) :]
    64   1731.9 MiB      0.0 MiB           8               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    65                                                 ]
    66   1731.9 MiB      0.0 MiB           4           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    67   1731.9 MiB      0.0 MiB           4           responses.append(response)
    68   1731.9 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 118.49s)

[TIMING] _generate_responses_batch - START
Filename: /home/hy676/cs5416-final-project/exp3/04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    31   1732.7 MiB   1732.7 MiB           1   @profile_with_timing
    32                                         @profile
    33                                         def _generate_responses_batch(
    34                                             queries: list[str], documents_batch: list[list[dict]]
    35                                         ) -> list[str]:
    36                                             """Step 6: Generate LLM responses for each query in the batch"""
    37   1732.7 MiB      0.0 MiB           1       responses = []
    38   1750.2 MiB      0.0 MiB           5       for query, documents in zip(queries, documents_batch):
    39   1746.8 MiB      0.0 MiB           8           context = "\n".join(
    40   1746.8 MiB      0.0 MiB          24               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    41                                                 )
    42   1746.8 MiB      0.0 MiB           4           messages = [
    43   1746.8 MiB      0.0 MiB           4               {
    44   1746.8 MiB      0.0 MiB           4                   "role": "system",
    45   1746.8 MiB      0.0 MiB           4                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    46                                                     },
    47   1746.8 MiB      0.0 MiB           4               {
    48   1746.8 MiB      0.0 MiB           4                   "role": "user",
    49   1746.8 MiB      0.0 MiB           4                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    50                                                     },
    51                                                 ]
    52   1746.8 MiB      0.0 MiB           8           text = tokenizer.apply_chat_template(
    53   1746.8 MiB      0.0 MiB           4               messages, tokenize=False, add_generation_prompt=True
    54                                                 )
    55   1746.8 MiB      0.0 MiB           4           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    56   1750.2 MiB     17.5 MiB          12           generated_ids = model.generate(
    57   1746.8 MiB      0.0 MiB           4               **model_inputs,
    58   1746.8 MiB      0.0 MiB           4               max_new_tokens=CONFIG["max_tokens"],
    59   1746.8 MiB      0.0 MiB           4               temperature=0.01,
    60   1746.8 MiB      0.0 MiB           4               pad_token_id=tokenizer.eos_token_id,
    61                                                 )
    62   1750.2 MiB      0.0 MiB          24           generated_ids = [
    63   1750.2 MiB      0.0 MiB           4               output_ids[len(input_ids) :]
    64   1750.2 MiB      0.0 MiB           8               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    65                                                 ]
    66   1750.2 MiB      0.0 MiB           4           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    67   1750.2 MiB      0.0 MiB           4           responses.append(response)
    68   1750.2 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 89.07s)
