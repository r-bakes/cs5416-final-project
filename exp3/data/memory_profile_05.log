============================================================
SENTIMENT & SAFETY SERVICE
============================================================
Node: 0
Port: 8005
Sentiment Model: nlptown/bert-base-multilingual-uncased-sentiment
Safety Model: unitary/toxic-bert
============================================================
 * Serving Flask app '05_sentiment_and_safety_service'
 * Debug mode: off

[TIMING] _analyze_sentiment_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     24.5 MiB     24.5 MiB           1   @profile_with_timing
    25                                         @profile
    26                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    27                                             """Step 7: Analyze sentiment for each generated response"""
    28     24.6 MiB      0.1 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    29    394.7 MiB    370.1 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    30    394.7 MiB      0.0 MiB           1       sentiment_map = {
    31    394.7 MiB      0.0 MiB           1           "1 star": "very negative",
    32    394.7 MiB      0.0 MiB           1           "2 stars": "negative",
    33    394.7 MiB      0.0 MiB           1           "3 stars": "neutral",
    34    394.7 MiB      0.0 MiB           1           "4 stars": "positive",
    35    394.7 MiB      0.0 MiB           1           "5 stars": "very positive",
    36                                             }
    37    394.7 MiB      0.0 MiB           1       sentiments = []
    38    394.7 MiB      0.0 MiB           2       for result in raw_results:
    39    394.7 MiB      0.0 MiB           1           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    40    394.7 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 0.94s)

[TIMING] _filter_response_safety_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    42    394.9 MiB    394.9 MiB           1   @profile_with_timing
    43                                         @profile
    44                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    45                                             """Step 8: Filter responses for safety for each entry in the batch"""
    46    394.9 MiB      0.0 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    47    725.3 MiB    330.5 MiB           1       raw_results = safety_classifier(truncated_texts)
    48    725.3 MiB      0.0 MiB           1       toxicity_flags = []
    49    725.3 MiB      0.0 MiB           2       for result in raw_results:
    50    725.3 MiB      0.0 MiB           1           toxicity_flags.append(result["score"] > 0.5)
    51    725.3 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 0.97s)

[TIMING] _analyze_sentiment_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     31.0 MiB     31.0 MiB           1   @profile_with_timing
    25                                         @profile
    26                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    27                                             """Step 7: Analyze sentiment for each generated response"""
    28     31.1 MiB      0.1 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    29    384.8 MiB    353.7 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    30    384.8 MiB      0.0 MiB           1       sentiment_map = {
    31    384.8 MiB      0.0 MiB           1           "1 star": "very negative",
    32    384.8 MiB      0.0 MiB           1           "2 stars": "negative",
    33    384.8 MiB      0.0 MiB           1           "3 stars": "neutral",
    34    384.8 MiB      0.0 MiB           1           "4 stars": "positive",
    35    384.8 MiB      0.0 MiB           1           "5 stars": "very positive",
    36                                             }
    37    384.8 MiB      0.0 MiB           1       sentiments = []
    38    384.8 MiB      0.0 MiB           2       for result in raw_results:
    39    384.8 MiB      0.0 MiB           1           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    40    384.8 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 0.96s)

[TIMING] _filter_response_safety_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    42    384.9 MiB    384.9 MiB           1   @profile_with_timing
    43                                         @profile
    44                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    45                                             """Step 8: Filter responses for safety for each entry in the batch"""
    46    384.9 MiB      0.0 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    47    714.0 MiB    329.0 MiB           1       raw_results = safety_classifier(truncated_texts)
    48    714.0 MiB      0.0 MiB           1       toxicity_flags = []
    49    714.0 MiB      0.0 MiB           2       for result in raw_results:
    50    714.0 MiB      0.0 MiB           1           toxicity_flags.append(result["score"] > 0.5)
    51    714.0 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 0.89s)

[TIMING] _analyze_sentiment_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     29.4 MiB     29.4 MiB           1   @profile_with_timing
    25                                         @profile
    26                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    27                                             """Step 7: Analyze sentiment for each generated response"""
    28     29.5 MiB      0.1 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    29    392.0 MiB    362.5 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    30    392.0 MiB      0.0 MiB           1       sentiment_map = {
    31    392.0 MiB      0.0 MiB           1           "1 star": "very negative",
    32    392.0 MiB      0.0 MiB           1           "2 stars": "negative",
    33    392.0 MiB      0.0 MiB           1           "3 stars": "neutral",
    34    392.0 MiB      0.0 MiB           1           "4 stars": "positive",
    35    392.0 MiB      0.0 MiB           1           "5 stars": "very positive",
    36                                             }
    37    392.0 MiB      0.0 MiB           1       sentiments = []
    38    392.0 MiB      0.0 MiB           2       for result in raw_results:
    39    392.0 MiB      0.0 MiB           1           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    40    392.0 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 1.10s)

[TIMING] _filter_response_safety_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    42    392.1 MiB    392.1 MiB           1   @profile_with_timing
    43                                         @profile
    44                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    45                                             """Step 8: Filter responses for safety for each entry in the batch"""
    46    392.1 MiB      0.0 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    47    719.9 MiB    327.8 MiB           1       raw_results = safety_classifier(truncated_texts)
    48    719.9 MiB      0.0 MiB           1       toxicity_flags = []
    49    719.9 MiB      0.0 MiB           2       for result in raw_results:
    50    719.9 MiB      0.0 MiB           1           toxicity_flags.append(result["score"] > 0.5)
    51    719.9 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 0.07s)
