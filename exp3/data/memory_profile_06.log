============================================================
SENTIMENT & SAFETY SERVICE
============================================================
Node: 0
Port: 8006
Sentiment Model: nlptown/bert-base-multilingual-uncased-sentiment
Safety Model: unitary/toxic-bert
============================================================
 * Serving Flask app '05_sentiment_and_safety_service'
 * Debug mode: off

[TIMING] _analyze_sentiment_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     24.6 MiB     24.6 MiB           1   @profile_with_timing
    25                                         @profile
    26                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    27                                             """Step 7: Analyze sentiment for each generated response"""
    28     24.7 MiB      0.1 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    29    391.9 MiB    367.2 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    30    391.9 MiB      0.0 MiB           1       sentiment_map = {
    31    391.9 MiB      0.0 MiB           1           "1 star": "very negative",
    32    391.9 MiB      0.0 MiB           1           "2 stars": "negative",
    33    391.9 MiB      0.0 MiB           1           "3 stars": "neutral",
    34    391.9 MiB      0.0 MiB           1           "4 stars": "positive",
    35    391.9 MiB      0.0 MiB           1           "5 stars": "very positive",
    36                                             }
    37    391.9 MiB      0.0 MiB           1       sentiments = []
    38    391.9 MiB      0.0 MiB           2       for result in raw_results:
    39    391.9 MiB      0.0 MiB           1           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    40    391.9 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 0.95s)

[TIMING] _filter_response_safety_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    42    392.0 MiB    392.0 MiB           1   @profile_with_timing
    43                                         @profile
    44                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    45                                             """Step 8: Filter responses for safety for each entry in the batch"""
    46    392.0 MiB      0.0 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    47    721.8 MiB    329.8 MiB           1       raw_results = safety_classifier(truncated_texts)
    48    721.8 MiB      0.0 MiB           1       toxicity_flags = []
    49    721.8 MiB      0.0 MiB           2       for result in raw_results:
    50    721.8 MiB      0.0 MiB           1           toxicity_flags.append(result["score"] > 0.5)
    51    721.8 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 1.01s)

[TIMING] _analyze_sentiment_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     30.7 MiB     30.7 MiB           1   @profile_with_timing
    25                                         @profile
    26                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    27                                             """Step 7: Analyze sentiment for each generated response"""
    28     30.8 MiB      0.1 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    29    388.2 MiB    357.4 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    30    388.2 MiB      0.0 MiB           1       sentiment_map = {
    31    388.2 MiB      0.0 MiB           1           "1 star": "very negative",
    32    388.2 MiB      0.0 MiB           1           "2 stars": "negative",
    33    388.2 MiB      0.0 MiB           1           "3 stars": "neutral",
    34    388.2 MiB      0.0 MiB           1           "4 stars": "positive",
    35    388.2 MiB      0.0 MiB           1           "5 stars": "very positive",
    36                                             }
    37    388.2 MiB      0.0 MiB           1       sentiments = []
    38    388.2 MiB      0.0 MiB           2       for result in raw_results:
    39    388.2 MiB      0.0 MiB           1           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    40    388.2 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 1.02s)

[TIMING] _filter_response_safety_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    42    388.3 MiB    388.3 MiB           1   @profile_with_timing
    43                                         @profile
    44                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    45                                             """Step 8: Filter responses for safety for each entry in the batch"""
    46    388.3 MiB      0.0 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    47    717.8 MiB    329.6 MiB           1       raw_results = safety_classifier(truncated_texts)
    48    717.8 MiB      0.0 MiB           1       toxicity_flags = []
    49    717.8 MiB      0.0 MiB           2       for result in raw_results:
    50    717.8 MiB      0.0 MiB           1           toxicity_flags.append(result["score"] > 0.5)
    51    717.8 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 0.99s)

[TIMING] _analyze_sentiment_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    293.8 MiB    293.8 MiB           1   @profile_with_timing
    25                                         @profile
    26                                         def _analyze_sentiment_batch(texts: list[str]) -> list[str]:
    27                                             """Step 7: Analyze sentiment for each generated response"""
    28    293.9 MiB      0.1 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    29    507.8 MiB    213.8 MiB           1       raw_results = sentiment_classifier(truncated_texts)
    30    507.8 MiB      0.0 MiB           1       sentiment_map = {
    31    507.8 MiB      0.0 MiB           1           "1 star": "very negative",
    32    507.8 MiB      0.0 MiB           1           "2 stars": "negative",
    33    507.8 MiB      0.0 MiB           1           "3 stars": "neutral",
    34    507.8 MiB      0.0 MiB           1           "4 stars": "positive",
    35    507.8 MiB      0.0 MiB           1           "5 stars": "very positive",
    36                                             }
    37    507.8 MiB      0.0 MiB           1       sentiments = []
    38    507.8 MiB      0.0 MiB           2       for result in raw_results:
    39    507.8 MiB      0.0 MiB           1           sentiments.append(sentiment_map.get(result["label"], "neutral"))
    40    507.8 MiB      0.0 MiB           1       return sentiments


[TIMING] _analyze_sentiment_batch - END (took 0.96s)

[TIMING] _filter_response_safety_batch - START
Filename: /Users/laurence/Documents/Cornell/CS5416/cs5416-final-project/exp3/05_sentiment_and_safety_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    42    507.8 MiB    507.8 MiB           1   @profile_with_timing
    43                                         @profile
    44                                         def _filter_response_safety_batch(texts: list[str]) -> list[bool]:
    45                                             """Step 8: Filter responses for safety for each entry in the batch"""
    46    507.8 MiB      0.0 MiB           2       truncated_texts = [text[: CONFIG["truncate_length"]] for text in texts]
    47    712.4 MiB    204.6 MiB           1       raw_results = safety_classifier(truncated_texts)
    48    712.4 MiB      0.0 MiB           1       toxicity_flags = []
    49    712.4 MiB      0.0 MiB           2       for result in raw_results:
    50    712.4 MiB      0.0 MiB           1           toxicity_flags.append(result["score"] > 0.5)
    51    712.4 MiB      0.0 MiB           1       return toxicity_flags


[TIMING] _filter_response_safety_batch - END (took 0.80s)
