Loading LLM model...
LLM model loaded!
============================================================
LLM SERVICE
============================================================
Node: 1
Port: 8009
Model: Qwen/Qwen2.5-0.5B-Instruct
============================================================
 * Serving Flask app '04_llm_service'
 * Debug mode: off

[TIMING] _generate_responses_batch - START
Filename: G:\cs5416-final-project\exp3\04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    29     35.9 MiB     35.9 MiB           1   @profile_with_timing
    30                                         @profile
    31                                         def _generate_responses_batch(
    32                                             queries: list[str], documents_batch: list[list[dict]]
    33                                         ) -> list[str]:
    34                                             """Step 6: Generate LLM responses for each query in the batch"""
    35     36.0 MiB      0.1 MiB           1       responses = []
    36   1029.5 MiB      0.0 MiB           2       for query, documents in zip(queries, documents_batch):
    37     36.0 MiB      0.0 MiB           2           context = "\n".join(
    38     36.0 MiB      0.0 MiB           4               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    39                                                 )
    40     36.0 MiB      0.0 MiB           1           messages = [
    41     36.0 MiB      0.0 MiB           1               {
    42     36.0 MiB      0.0 MiB           1                   "role": "system",
    43     36.0 MiB      0.0 MiB           1                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    44                                                     },
    45     36.0 MiB      0.0 MiB           1               {
    46     36.0 MiB      0.0 MiB           1                   "role": "user",
    47     36.0 MiB      0.0 MiB           1                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    48                                                     },
    49                                                 ]
    50     47.5 MiB     11.5 MiB           2           text = tokenizer.apply_chat_template(
    51     36.1 MiB      0.0 MiB           1               messages, tokenize=False, add_generation_prompt=True
    52                                                 )
    53     56.4 MiB      8.9 MiB           1           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    54   1028.6 MiB    972.2 MiB           3           generated_ids = model.generate(
    55     56.4 MiB      0.0 MiB           1               **model_inputs,
    56     56.4 MiB      0.0 MiB           1               max_new_tokens=CONFIG["max_tokens"],
    57     56.4 MiB      0.0 MiB           1               temperature=0.01,
    58     56.4 MiB      0.0 MiB           1               pad_token_id=tokenizer.eos_token_id,
    59                                                 )
    60   1028.7 MiB      0.0 MiB           2           generated_ids = [
    61   1028.7 MiB      0.0 MiB           1               output_ids[len(input_ids) :]
    62   1028.7 MiB      0.1 MiB           3               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    63                                                 ]
    64   1029.5 MiB      0.8 MiB           1           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    65   1029.5 MiB      0.0 MiB           1           responses.append(response)
    66   1029.5 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 11.44s)

[TIMING] _generate_responses_batch - START
Filename: G:\cs5416-final-project\exp3\04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    29     20.1 MiB     20.1 MiB           1   @profile_with_timing
    30                                         @profile
    31                                         def _generate_responses_batch(
    32                                             queries: list[str], documents_batch: list[list[dict]]
    33                                         ) -> list[str]:
    34                                             """Step 6: Generate LLM responses for each query in the batch"""
    35     20.1 MiB      0.1 MiB           1       responses = []
    36   1001.6 MiB      0.0 MiB           2       for query, documents in zip(queries, documents_batch):
    37     20.2 MiB      0.0 MiB           2           context = "\n".join(
    38     20.2 MiB      0.0 MiB           4               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    39                                                 )
    40     20.2 MiB      0.0 MiB           1           messages = [
    41     20.2 MiB      0.0 MiB           1               {
    42     20.2 MiB      0.0 MiB           1                   "role": "system",
    43     20.2 MiB      0.0 MiB           1                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    44                                                     },
    45     20.2 MiB      0.0 MiB           1               {
    46     20.2 MiB      0.0 MiB           1                   "role": "user",
    47     20.2 MiB      0.0 MiB           1                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    48                                                     },
    49                                                 ]
    50     21.5 MiB      1.4 MiB           2           text = tokenizer.apply_chat_template(
    51     20.2 MiB      0.0 MiB           1               messages, tokenize=False, add_generation_prompt=True
    52                                                 )
    53     27.3 MiB      5.8 MiB           1           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    54   1001.0 MiB    973.6 MiB           3           generated_ids = model.generate(
    55     27.4 MiB      0.0 MiB           1               **model_inputs,
    56     27.4 MiB      0.0 MiB           1               max_new_tokens=CONFIG["max_tokens"],
    57     27.4 MiB      0.0 MiB           1               temperature=0.01,
    58     27.4 MiB      0.0 MiB           1               pad_token_id=tokenizer.eos_token_id,
    59                                                 )
    60   1001.1 MiB      0.0 MiB           2           generated_ids = [
    61   1001.1 MiB      0.0 MiB           1               output_ids[len(input_ids) :]
    62   1001.1 MiB      0.1 MiB           3               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    63                                                 ]
    64   1001.6 MiB      0.5 MiB           1           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    65   1001.6 MiB      0.0 MiB           1           responses.append(response)
    66   1001.6 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 4.84s)

[TIMING] _generate_responses_batch - START
Filename: G:\cs5416-final-project\exp3\04_llm_service.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    29     17.9 MiB     17.9 MiB           1   @profile_with_timing
    30                                         @profile
    31                                         def _generate_responses_batch(
    32                                             queries: list[str], documents_batch: list[list[dict]]
    33                                         ) -> list[str]:
    34                                             """Step 6: Generate LLM responses for each query in the batch"""
    35     18.0 MiB      0.1 MiB           1       responses = []
    36    996.5 MiB      0.0 MiB           2       for query, documents in zip(queries, documents_batch):
    37     18.0 MiB      0.0 MiB           2           context = "\n".join(
    38     18.0 MiB      0.0 MiB           4               [f"- {doc['title']}: {doc['content'][:200]}" for doc in documents[:3]]
    39                                                 )
    40     18.0 MiB      0.0 MiB           1           messages = [
    41     18.0 MiB      0.0 MiB           1               {
    42     18.0 MiB      0.0 MiB           1                   "role": "system",
    43     18.0 MiB      0.0 MiB           1                   "content": "When given Context and Question, reply as 'Answer: <final answer>' only.",
    44                                                     },
    45     18.0 MiB      0.0 MiB           1               {
    46     18.0 MiB      0.0 MiB           1                   "role": "user",
    47     18.0 MiB      0.0 MiB           1                   "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
    48                                                     },
    49                                                 ]
    50     19.3 MiB      1.3 MiB           2           text = tokenizer.apply_chat_template(
    51     18.0 MiB      0.0 MiB           1               messages, tokenize=False, add_generation_prompt=True
    52                                                 )
    53     24.9 MiB      5.6 MiB           1           model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    54    996.2 MiB    971.3 MiB           3           generated_ids = model.generate(
    55     24.9 MiB      0.0 MiB           1               **model_inputs,
    56     24.9 MiB      0.0 MiB           1               max_new_tokens=CONFIG["max_tokens"],
    57     24.9 MiB      0.0 MiB           1               temperature=0.01,
    58     24.9 MiB      0.0 MiB           1               pad_token_id=tokenizer.eos_token_id,
    59                                                 )
    60    996.3 MiB      0.0 MiB           2           generated_ids = [
    61    996.3 MiB      0.0 MiB           1               output_ids[len(input_ids) :]
    62    996.3 MiB      0.1 MiB           3               for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    63                                                 ]
    64    996.5 MiB      0.2 MiB           1           response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    65    996.5 MiB      0.0 MiB           1           responses.append(response)
    66    996.5 MiB      0.0 MiB           1       return responses


[TIMING] _generate_responses_batch - END (took 2.27s)
